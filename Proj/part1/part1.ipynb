{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries and Modules here...\n",
    "import spacy\n",
    "import math\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        ## You should use these variable to store the term frequencies for tokens and entities...\n",
    "        self.tf_tokens = {}\n",
    "        self.tf_entities = {}\n",
    "\n",
    "        ## You should use these variable to store the inverse document frequencies for tokens and entities...\n",
    "        self.idf_tokens = {}\n",
    "        self.idf_entities = {}\n",
    "\n",
    "    ## Your implementation for indexing the documents...\n",
    "    def index_documents(self, documents):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        token = {}\n",
    "        entity = {}\n",
    "        idf_tokens = {}\n",
    "        idf_entities = {}\n",
    "        l = len(documents)\n",
    "        for i in documents:\n",
    "            entities = [str(ett) for ett in nlp(documents[i]).ents if nlp.vocab[str(ett)].is_stop == False]\n",
    "            doc = [token.orth_ for token in nlp(documents[i]) if token.is_punct == False and token.is_space == False and token.is_stop == False]                    \n",
    "            for k in entities:\n",
    "                if k not in entity:\n",
    "                    entity[k] = {i:1}\n",
    "                else:\n",
    "                    if i not in entity[k]:\n",
    "                        entity[k][i] = 1\n",
    "                    else:\n",
    "                        entity[k][i] += 1\n",
    "            for j in doc:\n",
    "                if j in entities:\n",
    "                    n = doc.count(j) - entities.count(j)\n",
    "                    if n != 0: \n",
    "                        if j not in token:\n",
    "                            token[j] = {i:n}\n",
    "                        else:\n",
    "                            token[j][i] = n\n",
    "                else:\n",
    "                    if j not in token:\n",
    "                        token[j] = {i:1}\n",
    "                    else:\n",
    "                        if i not in token[j]:\n",
    "                            token[j][i] = 1\n",
    "                        else:\n",
    "                            token[j][i] += 1\n",
    "        \n",
    "\n",
    "        for p in token:\n",
    "            idf_tokens[p] = 1 + math.log(l/(1+len(token[p])))\n",
    "        for q in entity:\n",
    "                idf_entities[q] = 1 + math.log(l/(1+len(entity[q])))\n",
    "        \n",
    "        self.tf_tokens = token\n",
    "        self.tf_entities = entity\n",
    "        self.idf_tokens = idf_tokens\n",
    "        self.idf_entities = idf_entities\n",
    "\n",
    "    ## Your implementation to split the query to tokens and entities...\n",
    "    def split_query(self, Q, DoE):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        q = [token.orth_ for token in nlp(Q) if not token.is_punct | token.is_space]\n",
    "        query = [[[],q]]\n",
    "        newq = [[[],q]]\n",
    "\n",
    "        while newq != []:\n",
    "            left = []\n",
    "            for [e,k] in newq:\n",
    "                for i in DoE:\n",
    "                    words = i.split()\n",
    "                    l = len(words)\n",
    "                    nb = 0\n",
    "                    for j in range(l):\n",
    "                        if words[j] in k:\n",
    "                            nb += 1\n",
    "                    if nb == l:\n",
    "                        newk = k[:]\n",
    "                        for w in words:\n",
    "                            newk.remove(newk[newk.index(w)])\n",
    "                        if e != []:\n",
    "                            if Q.index(words[0]) > Q.index(e[-1].split()[0]):\n",
    "                                newe = e[:]\n",
    "                                newe.append(i)\n",
    "                                query.append([newe,newk])\n",
    "                                left.append([newe,newk])\n",
    "                        else:\n",
    "                            newe = e[:]\n",
    "                            newe.append(i)\n",
    "                            query.append([newe,newk])\n",
    "                            left.append([newe,newk])\n",
    "\n",
    "            newq = left\n",
    "\n",
    "        return query\n",
    "    ## Your implementation to return the max score among all the query splits...\n",
    "    def max_score_query(self, query_splits, doc_id):\n",
    "        s = 0\n",
    "        max_score = 0\n",
    "        for [e,k] in query_splits:\n",
    "            s1 = 0\n",
    "            s2 = 0\n",
    "            if e != []:\n",
    "                for i in e:\n",
    "                    if i in self.tf_entities and doc_id in self.tf_entities[i]:\n",
    "                        tf_e = self.tf_entities[i][doc_id]\n",
    "                        tf_ne = 1 + math.log(tf_e)\n",
    "                        idf = self.idf_entities[i]\n",
    "                        s1 += idf * tf_ne\n",
    "            if k != []:\n",
    "                for j in k:\n",
    "                    if j in self.tf_tokens and doc_id in self.tf_tokens[j]:\n",
    "                        tf_k = self.tf_tokens[j][doc_id]\n",
    "                        tf_nk = 1 + math.log(1 + math.log(tf_k))\n",
    "                        idf = self.idf_tokens[j]\n",
    "                        s2 += idf * tf_nk\n",
    "\n",
    "            s = s1 + 0.4*s2\n",
    "            if s > max_score:\n",
    "                max_score = s\n",
    "                tk = k\n",
    "                et = e\n",
    "        result = (max_score,{'tokens':tk,'entities':et})\n",
    "        \n",
    "        return result\n",
    "        ## Output should be a tuple (max_score, {'tokens': [...], 'entities': [...]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {1:'President Trump was on his way to new New York in New York City.',\n",
    "             2:'New York Times mentioned an interesting story about Trump.',\n",
    "             3:'I think it would be great if I can travel to New York this summer to see Trump.'}\n",
    "Q = 'Los The Angeles Boston Times Globe Washington Post'\n",
    "DoE = {'Los Angeles Times':0, 'The Boston Globe':1,'The Washington Post':2, 'Star Tribune':3}\n",
    "doc_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trump,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('Trump').ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'a']\n",
      "['a']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "a = ['b','a','b','c','d','a']\n",
    "c = ['b','d','a']\n",
    "d = a[:]\n",
    "n = 0\n",
    "while d != []:\n",
    "    if c[n] in d:\n",
    "        d = d[d.index(c[n])+1:]\n",
    "        n += 1\n",
    "        print(d)\n",
    "    else:\n",
    "        d = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['b','a','b','c','d','a']\n",
    "\n",
    "a = 'The New New York City Times of India'.split()\n",
    "aa = a[:]\n",
    "c = ['The New York Times']\n",
    "c = c[0].split()\n",
    "ind = 0\n",
    "l = []\n",
    "for w in c:\n",
    "    ind = aa.index(w)\n",
    "    l.append(aa[:ind])\n",
    "    aa = aa[ind+1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New', 'City', 'of', 'India']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = []\n",
    "for i in l:\n",
    "    list += i\n",
    "list += aa\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict((i,dict((n,a.count(i)) for n in range(3))) for i in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'a']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['a','b','c']\n",
    "b=['a']\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[] [ABCD]\n",
    "[AD] [BC]\n",
    "[AD BC] []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
